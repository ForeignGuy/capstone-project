# -*- coding: utf-8 -*-
# """Untitled1.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
    # https://colab.research.google.com/drive/1af9JtiVXDpReTj4dqaKeJ-VCrFIAptYj
# """
import sys
# from google.colab import files
# uploaded = files.upload()
# import regex._regex as _regex
import numpy as np
import torch.utils.data as data_utils
import torch.optim as optim
import gc
from tqdm import tqdm
# import torch
import torch.nn as nn
# import keras
import torch
import pandas as pd
# import itertools
# from sklearn.model_selection import train_test_split
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.linear_model import PassiveAggressiveClassifier
# from sklearn.metrics import accuracy_score, confusion_matrix

from transformers import BertForSequenceClassification, BertTokenizer
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# inputVar = sys.argv[1]
# print(inputVar)
inputVar = "KANSAS CITY, MO—Acknowledging that she had seen all the warnings about holiday travel on the news, local mom Mary Simpson announced Tuesday that she completely understood that coming to Thanksgiving would be risky for all involved and that you didn’t love her anymore. “No, no, don’t apologize, honey—if it isn’t safe and you don’t care enough about me to make the trip, you should probably just stay home this year,” Simpson said in between audibly heavy sighs, adding that if anyone understood it was her, what with her migraines and your father’s bad knee making their last trip to visit you so difficult, though they obviously wouldn’t have missed it for the world. “I’m quite aware of the public health situation, so there’s no need to explain why you think it might be too tricky to navigate travel when you clearly have a life of your own with no room in it for a boring old lady like me. After all, we’re living in difficult times, and I’m sure the last thing you want to do right now is come see the person who gave you life and loves you more than anything in the world. I’ll just be here in this big, quiet house with your dad and no one else, if you’re saying that’s the safest way to celebrate.” Simpson went on to remark that she might as well not bother cooking a Thanksgiving meal this year, seeing as there would be far too many leftovers to deal with if you weren’t coming."
# Read the data
df = pd.read_csv("newsUpdated.csv")
data = {'text':  [inputVar],
        'label': ['FAKE'],
        'goal': [0]}
dfinput = pd.DataFrame(data, columns=['text', 'label', 'goal'])
dfinput.shape
#Get shape and head
df.shape
df.columns = ['text', 'label', 'goal']
# del df['id']
# del df['title']
dfinput.columns = ['text', 'label', 'goal']
df.head(10)

dfinput.head()

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], df['text']))

tokenized_dfinput = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], dfinput['text']))

totalpadlength = 512

indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, tokenized_df))

indexed_tokensinput = list(map(tokenizer.convert_tokens_to_ids, tokenized_dfinput))

index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in indexed_tokens])

index_paddedinput = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in indexed_tokensinput])

#DataFlair - Get the labels
labelst = df['goal'].values
texts=df.text
texts.head()

#DataFlair - Get the labels
labelstinput = dfinput['goal'].values
texts=dfinput.text
texts.head()



all_words = []
for l in tokenized_df:
  all_words.extend(l)
all_indices = []
for i in indexed_tokens:
  all_indices.extend(i)

word_to_ix = dict(zip(all_words, all_indices))
ix_to_word = dict(zip(all_indices, all_words))

all_words = []
for l in tokenized_dfinput:
  all_words.extend(l)
all_indices = []
for i in indexed_tokens:
  all_indices.extend(i)

word_to_ix = dict(zip(all_words, all_indices))
ix_to_word = dict(zip(all_indices, all_words))

mask_variable = [[float(i>0) for i in ii] for ii in index_padded]

mask_variableinput = [[float(i>0) for i in ii] for ii in index_paddedinput]

BATCH_SIZE = 2
def format_tensors(text_data, mask, labels, batch_size):
    X = torch.from_numpy(text_data)
    X = X.long()
    mask = torch.tensor(mask)
    y = torch.from_numpy(labels)
    y = y.long()
    tensordata = data_utils.TensorDataset(X, mask, y)
    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)
    return loader

X_train = index_padded
y_train = labelst

X_test = index_paddedinput
y_test = labelstinput

train_masks = mask_variable
test_masks = mask_variableinput

trainloader = format_tensors(X_train, train_masks, y_train,BATCH_SIZE)
testloader = format_tensors(X_test, test_masks, y_test, BATCH_SIZE)

model = BertForSequenceClassification.from_pretrained('bert-base-cased')
model

def compute_accuracy(model, dataloader, device):
    tqdm()
    model.eval()
    correct_preds, num_samples = 0,0
    with torch.no_grad():
        for i, batch in enumerate(tqdm(dataloader)):
            token_ids, masks, labels = tuple(t.to(device) for t in batch)
            _, yhat = model(input_ids=token_ids, attention_mask=masks, labels=labels)
            prediction = (torch.sigmoid(yhat[:,1]) > 0.5).long()
            num_samples += labels.size(0)
            correct_preds += (prediction==labels.long()).sum()
            del token_ids, masks, labels #memory
        torch.cuda.empty_cache() #memory
        gc.collect() # memory
        return correct_preds.float()/num_samples*100

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")
torch.cuda.empty_cache() #memory
gc.collect() #memory
NUM_EPOCHS = 1
loss_function = nn.BCEWithLogitsLoss()
losses = []
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=3e-6)
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    iteration = 0
    for i, batch in enumerate(trainloader):
        iteration += 1
        token_ids, masks, labels = tuple(t.to(device) for t in batch)
        optimizer.zero_grad()
        loss, yhat = model(input_ids=token_ids, attention_mask=masks, labels=labels)
        loss.backward()
        optimizer.step()
        running_loss += float(loss.item())
        del token_ids, masks, labels #memory

        if not i%25:
            print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '
                  f'Batch {i+1:03d}/{len(trainloader):03d} | '
                  f'Average Loss in last {iteration} iteration(s): {(running_loss/iteration):.4f}')
            running_loss = 0.0
            iteration = 0
        torch.cuda.empty_cache() #memory
        gc.collect() #memory
        losses.append(float(loss.item()))
    with torch.set_grad_enabled(False):
        print(f'\nTraining Accuracy: '
              f'{compute_accuracy(model, trainloader, device):.2f}%')

# with torch.set_grad_enabled(False):
  # print(f'\n\nTest Accuracy:'
  # f'{compute_accuracy(model, testloader, device):.2f}%')

test_predictions = torch.zeros((len(y_test),1))

test_predictions_percent = torch.zeros((len(y_test),1))
with torch.no_grad():
  for i, batch in enumerate(tqdm(testloader)):
    token_ids, masks, labels = tuple(t.to(device) for t in batch)
    _, yhat = model(input_ids=token_ids, attention_mask=masks, labels=labels)
    prediction = (torch.sigmoid(yhat[:,1]) > 0.5).long().view(-1,1)
    test_predictions[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = prediction
    test_predictions_percent[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = torch.sigmoid(yhat[:,1]).view(-1,1)

percentcon = np.array(test_predictions_percent.reshape(-1), dtype=float).tolist()

print(percentcon)

# if percentcon > .50:
    # print("Marv is sure of this answer")
# else:
    # print("Marv is not sure of this answer")

guess = np.array(test_predictions.reshape(-1), dtype=int).tolist()

print(guess)
# if guess == 0:
    # print("FAKE")
# else:
    # print("REAL")
